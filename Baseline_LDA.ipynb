{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os, glob\n",
    "import string\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import joblib, multiprocessing\n",
    "import utils as my_utils\n",
    "\n",
    "from scipy import spatial\n",
    "from collections import Counter\n",
    "from scipy.special import gammaln\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = 5\n",
    "max_df = .5\n",
    "max_features = 50000\n",
    "\n",
    "n_cores = 10\n",
    "max_iter = 20\n",
    "n_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = glob.glob(\"datasets/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/amazon_home_20000_dataset\n",
      "25\n",
      "0.4271302085198412 -0.07371590027560043 9.242989748366956 -25.857186877536762 -2174235.587381761 1469.0166597318778\n",
      "datasets/amazon_movies_20000_dataset\n",
      "25\n",
      "0.4409421370210823 -0.06882561030861212 11.951344837048973 -23.532061086580203 -2260069.8443661346 2274.726145103683\n",
      "datasets/amazon_electronics_20000_dataset\n",
      "25\n",
      "0.45916268840771896 -0.06120290744754176 7.5281213350236795 -22.533250912797488 -2170470.98212741 1295.1332948017164\n",
      "datasets/amazon_kindle_20000_dataset\n",
      "25\n",
      "0.4510683336048458 -0.055130329595544786 10.892115939728226 -21.32837119049423 -2032921.3300296064 1621.7023759542858\n",
      "datasets/twitter_airline_9061_dataset\n",
      "15\n",
      "0.38436392190535473 -0.00814053267267209 7.196767405583188 -24.77351297973521 -368986.1031729426 517.7264774213326\n",
      "datasets/imdb_reviews_20000_dataset\n",
      "10\n",
      "0.4173425952489424 -0.011608944779453512 14.666484476589588 -16.268223616906205 -4046438.448057927 2534.3618591214254\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "\n",
    "    dataset = pd.read_pickle(dataset)\n",
    "    n_topics = 5 * dataset.sentiment.unique().shape[0]\n",
    "    print(n_topics)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                                 stop_words=\"english\", max_features=max_features,\n",
    "                                 max_df=max_df, min_df=min_df)\n",
    "\n",
    "    count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "\n",
    "    model = LatentDirichletAllocation(n_components=n_topics, max_iter=max_iter, n_jobs=n_cores, verbose=0)\n",
    "\n",
    "    dt_distribution = model.fit_transform(count_matrix)\n",
    "\n",
    "    topic_words = {}\n",
    "    for topic, comp in enumerate(model.components_):\n",
    "        word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "        topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "    sample_df = []\n",
    "    for topic, word in topic_words.items():\n",
    "        sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "\n",
    "    print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "          silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "          davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "          my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "          model.score(count_matrix),\n",
    "          model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"datasets/amazon_electronics_20000_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None,preprocessor=None,\n",
    "                             stop_words=\"english\", max_features=max_features,\n",
    "                             max_df=max_df, min_df=min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = vectorizer.fit_transform(dataset.text.tolist()).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dict(zip(words,np.arange(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=n_topics, max_iter=20, n_jobs=n_cores, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = {}\n",
    "for topic, comp in enumerate(model.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "sample_df = []\n",
    "for topic, word in topic_words.items():\n",
    "    sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "dt_distribution = model.transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000), \n",
    "      silhouette_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)),\n",
    "      my_utils.coherence_score(count_matrix, sample_df, vocabulary),\n",
    "      model.score(count_matrix),\n",
    "      model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, n_topics, 2000))\n",
    "# print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "# print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "# print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "# print(\"Perplexity:\", model.perplexity(count_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"H Score:\", my_utils.get_hscore_multi(dt_distribution, count_matrix, k, 3000))\n",
    "#     print(\"Log Likelihood:\", model.score(count_matrix))\n",
    "#     print(\"Perplexity:\", model.perplexity(count_matrix))\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "    \n",
    "# #     print my_utils.coherence_score(count_matrix, sample_df, vocabulary), \"\\t\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)), \"\\t\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_evaluations_multi(model):\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     h_score =  my_utils.get_hscore_multi(dt_distribution, count_matrix, k)\n",
    "#     likelihood =  model.score(count_matrix)\n",
    "#     perplexity = model.perplexity(count_matrix)\n",
    "#     coherance_score = my_utils.coherence_score(count_matrix, sample_df, vocabulary)\n",
    "#     silhouette = silhouette_score(count_matrix, dt_distribution.argmax(axis=1))\n",
    "#     return [h_score, likelihood, perplexity, coherance_score, silhouette]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, model in zip(topics_grid, models_dump):\n",
    "\n",
    "#     topic_words = {}\n",
    "#     for topic, comp in enumerate(model.components_):\n",
    "#         word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "#         topic_words[topic] = [words[i] for i in word_idx]\n",
    "\n",
    "#     sample_df = []\n",
    "#     for topic, word in topic_words.items():\n",
    "#         sample_df.append(', '.join(word).split(\", \"))\n",
    "\n",
    "#     dt_distribution = model.transform(count_matrix)\n",
    "\n",
    "#     print(\"\\nK:\", k)\n",
    "#     print(\"Running Metrics...\")\n",
    "#     print(\"Coherance Score:\", my_utils.coherence_score(count_matrix, sample_df, vocabulary))\n",
    "#     print(\"Silhouette Score:\", silhouette_score(count_matrix, dt_distribution.argmax(axis=1)))\n",
    "#     print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Davies Bouldin Score:\", davies_bouldin_score(count_matrix, dt_distribution.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"H-Score:\", my_utils.get_hscore(dt_distribution, count_matrix, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(dt_distribution)\n",
    "\n",
    "# X_embedded.shape\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter([i[0] for i in X_embedded], [i[1] for i in X_embedded], c=dt_distribution.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
